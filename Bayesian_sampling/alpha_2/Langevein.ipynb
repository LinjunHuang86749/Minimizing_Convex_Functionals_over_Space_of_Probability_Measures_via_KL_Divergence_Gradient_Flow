{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "M = 1000\n",
        "pretrain_factor = 9\n",
        "total_steps = 100\n",
        "step_size = 1e-2\n",
        "total_trails = 10\n",
        "set_seed = 114540\n"
      ],
      "metadata": {
        "id": "fGsd3lcDRD4h"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bMkYagsXL43g"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as TD\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import gc\n",
        "import copy\n",
        "import math\n",
        "from scipy.optimize import linear_sum_assignment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
        "\n",
        "target_pretrain = TD.MultivariateNormal(\n",
        "    torch.zeros(2).to(device), pretrain_factor * torch.eye(2).to(device))\n",
        "std_normal = TD.MultivariateNormal(\n",
        "    torch.zeros(2).to(device), torch.eye(2).to(device))"
      ],
      "metadata": {
        "id": "7uSIEZSwQ8UD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Target(nn.Module):\n",
        "    \"\"\"\n",
        "    Sample target distributions to test models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, prop_scale=torch.tensor(6.0), prop_shift=torch.tensor(-3.0)):\n",
        "        \"\"\"Constructor\n",
        "\n",
        "        Args:\n",
        "          prop_scale: Scale for the uniform proposal\n",
        "          prop_shift: Shift for the uniform proposal\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"prop_scale\", prop_scale)\n",
        "        self.register_buffer(\"prop_shift\", prop_shift)\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          z: value or batch of latent variable\n",
        "\n",
        "        Returns:\n",
        "          log probability of the distribution for z\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"The log probability is not implemented yet.\")\n",
        "\n",
        "    def rejection_sampling(self, num_steps=1):\n",
        "        \"\"\"Perform rejection sampling on image distribution\n",
        "\n",
        "        Args:\n",
        "          num_steps: Number of rejection sampling steps to perform\n",
        "\n",
        "        Returns:\n",
        "          Accepted samples\n",
        "        \"\"\"\n",
        "        eps = torch.rand(\n",
        "            (num_steps, self.n_dims),\n",
        "            dtype=self.prop_scale.dtype,\n",
        "            device=self.prop_scale.device,\n",
        "        )\n",
        "        z_ = self.prop_scale * eps + self.prop_shift\n",
        "        prob = torch.rand(\n",
        "            num_steps, dtype=self.prop_scale.dtype, device=self.prop_scale.device\n",
        "        )\n",
        "        prob_ = torch.exp(self.log_prob(z_) - self.max_log_prob)\n",
        "        accept = prob_ > prob\n",
        "        z = z_[accept, :]\n",
        "        return z\n",
        "\n",
        "    def sample(self, num_samples=1):\n",
        "        \"\"\"Sample from image distribution through rejection sampling\n",
        "\n",
        "        Args:\n",
        "          num_samples: Number of samples to draw\n",
        "\n",
        "        Returns:\n",
        "          Samples\n",
        "        \"\"\"\n",
        "        z = torch.zeros(\n",
        "            (0, self.n_dims), dtype=self.prop_scale.dtype, device=self.prop_scale.device\n",
        "        )\n",
        "        while len(z) < num_samples:\n",
        "            z_ = self.rejection_sampling(num_samples)\n",
        "            ind = np.min([len(z_), num_samples - len(z)])\n",
        "            z = torch.cat([z, z_[:ind, :]], 0)\n",
        "        return z"
      ],
      "metadata": {
        "id": "eFtNLw-W4ZmZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetDist(Target):\n",
        "    \"\"\"\n",
        "    Bimodal two-dimensional distribution\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.n_dims = 2\n",
        "        self.max_log_prob = 0.0\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        \"\"\"\n",
        "        ```\n",
        "        log(p) = - 1/2 * ((norm(z) - 2) / 0.2) ** 2\n",
        "                 + log(  exp(-1/2 * ((z[0] - 2) / 0.3) ** 2)\n",
        "                       + exp(-1/2 * ((z[0] + 2) / 0.3) ** 2))\n",
        "        ```\n",
        "\n",
        "        Args:\n",
        "          z: value or batch of latent variable\n",
        "\n",
        "        Returns:\n",
        "          log probability of the distribution for z\n",
        "        \"\"\"\n",
        "        log_prob = (\n",
        "            -1/4 * (torch.linalg.vector_norm(z, ord=2, dim=1)) ** 4\n",
        "            - np.log(5.568327996831707845284817982118835702013624390283243910753675818829745533647795702212177687384708494)\n",
        "        )\n",
        "        return log_prob\n"
      ],
      "metadata": {
        "id": "yvAMNcLm4ceW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objective_fun_loss_list_2d = np.zeros((total_trails, total_steps))\n",
        "x_all_data = torch.zeros((total_steps, M, 2))\n",
        "\n",
        "p = TargetDist()\n",
        "for trail_num in range(total_trails):\n",
        "  torch.manual_seed(set_seed + trail_num)\n",
        "\n",
        "  target_sample = p.sample(M)\n",
        "  # initial x sampled from rho_0\n",
        "  x = target_pretrain.sample((M,)).to(device)\n",
        "\n",
        "  objective_fun_loss_list = np.array([])\n",
        "\n",
        "  for t in tqdm(range(total_steps)):\n",
        "    l2_norm = torch.linalg.vector_norm(x, dim=1).reshape(-1,1).to(device)\n",
        "\n",
        "    x_new = x - torch.cat([l2_norm, l2_norm], 1).to(device) ** 2 * x * step_size + math.sqrt(2 *  step_size) * std_normal.sample((M,)).to(device)\n",
        "    x = x_new\n",
        "    x_all_data[t,:,:] = x\n",
        "\n",
        "  for t in tqdm(range(total_steps)):\n",
        "    x = x_all_data[t,:,:]\n",
        "    x_final = target_sample\n",
        "    x_data_rep = x.repeat(1,1,M).reshape(M,M,-1)\n",
        "    cost = torch.norm(x_data_rep - x_final, p = 1, dim = 2).cpu().numpy()\n",
        "    row_ind, col_ind = linear_sum_assignment(cost)\n",
        "    temp_loss = (cost[row_ind, col_ind].sum() / M )\n",
        "    objective_fun_loss_list = np.append(objective_fun_loss_list, temp_loss )\n",
        "\n",
        "  objective_fun_loss_list_2d[trail_num,:] = objective_fun_loss_list"
      ],
      "metadata": {
        "id": "zs2T32-JRm5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}