{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cktzMYGRvRxW"
      },
      "outputs": [],
      "source": [
        "eta = 5\n",
        "optimizer_lr = 1e-5 # try 5e-6\n",
        "\n",
        "total_trial_num = 10        # num trial\n",
        "n_max_mirror_iterations=25       # num mirror steps\n",
        "n_max_iterations=1000        # max inner steps\n",
        "patient_max = 1000          # max patient\n",
        "\n",
        "stopping_norm = 1e-4\n",
        "num_samples = 1000         # number of theta generated\n",
        "\n",
        "verbose = True\n",
        "epcoh_min = 200\n",
        "set_seed = 114540 #\n",
        "pretrain_factor = 9\n",
        "\n",
        "outer_lr_final_factor = 1e-1\n",
        "outer_eta_final_factor = 1e0\n",
        "import math\n",
        "outer_lr_factor = pow(math.e, math.log(outer_lr_final_factor)/n_max_mirror_iterations)\n",
        "outer_eta_factor = pow(math.e, math.log(outer_eta_final_factor)/n_max_mirror_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1tMGeAVeztY",
        "outputId": "d7bb2ed3-9e33-4460-f39d-5922ec50d9fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting normflows\n",
            "  Downloading normflows-1.7.2.tar.gz (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from normflows) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from normflows) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->normflows) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->normflows) (1.3.0)\n",
            "Building wheels for collected packages: normflows\n",
            "  Building wheel for normflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for normflows: filename=normflows-1.7.2-py2.py3-none-any.whl size=86917 sha256=99eaee971b506a7204f394756df316df5464a36a323564d87727708dff4ab5a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/a4/89/3e09f53a561355c45eccfebeffc07a0e34d36a3f41e3ef68a3\n",
            "Successfully built normflows\n",
            "Installing collected packages: normflows\n",
            "Successfully installed normflows-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install normflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BTGAsyICe9iB"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import torch\n",
        "import numpy as np\n",
        "import normflows as nf\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as TD\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import gc\n",
        "import copy\n",
        "import time\n",
        "from scipy.optimize import linear_sum_assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UD4fG8tffElv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(114514)\n",
        "# Move model on GPU if available\n",
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iLrvJtaaQU9h"
      },
      "outputs": [],
      "source": [
        "class Target(nn.Module):\n",
        "    \"\"\"\n",
        "    Sample target distributions to test models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, prop_scale=torch.tensor(6.0), prop_shift=torch.tensor(-3.0)):\n",
        "        \"\"\"Constructor\n",
        "\n",
        "        Args:\n",
        "          prop_scale: Scale for the uniform proposal\n",
        "          prop_shift: Shift for the uniform proposal\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"prop_scale\", prop_scale)\n",
        "        self.register_buffer(\"prop_shift\", prop_shift)\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          z: value or batch of latent variable\n",
        "\n",
        "        Returns:\n",
        "          log probability of the distribution for z\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"The log probability is not implemented yet.\")\n",
        "\n",
        "    def rejection_sampling(self, num_steps=1):\n",
        "        \"\"\"Perform rejection sampling on image distribution\n",
        "\n",
        "        Args:\n",
        "          num_steps: Number of rejection sampling steps to perform\n",
        "\n",
        "        Returns:\n",
        "          Accepted samples\n",
        "        \"\"\"\n",
        "        eps = torch.rand(\n",
        "            (num_steps, self.n_dims),\n",
        "            dtype=self.prop_scale.dtype,\n",
        "            device=self.prop_scale.device,\n",
        "        )\n",
        "        z_ = self.prop_scale * eps + self.prop_shift\n",
        "        prob = torch.rand(\n",
        "            num_steps, dtype=self.prop_scale.dtype, device=self.prop_scale.device\n",
        "        )\n",
        "        prob_ = torch.exp(self.log_prob(z_) - self.max_log_prob)\n",
        "        accept = prob_ > prob\n",
        "        z = z_[accept, :]\n",
        "        return z\n",
        "\n",
        "    def sample(self, num_samples=1):\n",
        "        \"\"\"Sample from image distribution through rejection sampling\n",
        "\n",
        "        Args:\n",
        "          num_samples: Number of samples to draw\n",
        "\n",
        "        Returns:\n",
        "          Samples\n",
        "        \"\"\"\n",
        "        z = torch.zeros(\n",
        "            (0, self.n_dims), dtype=self.prop_scale.dtype, device=self.prop_scale.device\n",
        "        )\n",
        "        while len(z) < num_samples:\n",
        "            z_ = self.rejection_sampling(num_samples)\n",
        "            ind = np.min([len(z_), num_samples - len(z)])\n",
        "            z = torch.cat([z, z_[:ind, :]], 0)\n",
        "        return z\n",
        "\n",
        "class TargetDist(Target):\n",
        "    \"\"\"\n",
        "    Bimodal two-dimensional distribution\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.n_dims = 2\n",
        "        self.max_log_prob = 0.0\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        \"\"\"\n",
        "        ```\n",
        "        log(p) = - 1/2 * ((norm(z) - 2) / 0.2) ** 2\n",
        "                 + log(  exp(-1/2 * ((z[0] - 2) / 0.3) ** 2)\n",
        "                       + exp(-1/2 * ((z[0] + 2) / 0.3) ** 2))\n",
        "        ```\n",
        "\n",
        "        Args:\n",
        "          z: value or batch of latent variable\n",
        "\n",
        "        Returns:\n",
        "          log probability of the distribution for z\n",
        "        \"\"\"\n",
        "        log_prob = (\n",
        "            -1/4 * (torch.linalg.vector_norm(z, ord=2, dim=1)) ** 4\n",
        "            - np.log(math.pi ** (3/2))\n",
        "        )\n",
        "        return log_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR6v6kGrzqYf"
      },
      "outputs": [],
      "source": [
        "Ln_rho_k_list = np.zeros((total_trial_num, n_max_mirror_iterations))\n",
        "times_2d = np.zeros((total_trial_num, n_max_mirror_iterations))\n",
        "W1_2d = np.zeros((total_trial_num, n_max_mirror_iterations))\n",
        "\n",
        "dim1_normal = TD.MultivariateNormal(\n",
        "    torch.zeros(1).to(device), 1 * torch.eye(1).to(device))\n",
        "\n",
        "p = TargetDist()\n",
        "for trail_num in range(total_trial_num):\n",
        "\n",
        "  torch.manual_seed(set_seed + trail_num)\n",
        "\n",
        "  target_sample = p.sample(num_samples)\n",
        "\n",
        "  mirror_loss_hist = np.array([])\n",
        "  times = np.array([])\n",
        "  x_all_data = torch.zeros((n_max_mirror_iterations, num_samples, 2))\n",
        "\n",
        "  target_pretrain = TD.MultivariateNormal(\n",
        "      torch.zeros(2).to(device), pretrain_factor * torch.eye(2).to(device))\n",
        "  # Define 2D Gaussian base distribution\n",
        "  base = nf.distributions.DiagGaussian(2, trainable=False)\n",
        "  # Define list of flows\n",
        "  num_layers = 20\n",
        "  flows = []\n",
        "  for i in range(num_layers):\n",
        "      # Neural network with two hidden layers having 64 units each\n",
        "      # Last layer is initialized by zeros making training more stable\n",
        "      param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True )\n",
        "      # Add flow layer\n",
        "      flows.append(nf.flows.AffineCouplingBlock(param_map))\n",
        "      # Swap dimensions\n",
        "      flows.append(nf.flows.Permute(2, mode='swap'))\n",
        "\n",
        "\n",
        "  # Construct flow model\n",
        "  model = nf.NormalizingFlow(base, flows).to(device)\n",
        "  optimizer1 = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "  for it in tqdm(range(500)):\n",
        "      optimizer1.zero_grad()\n",
        "\n",
        "      # Get training samples\n",
        "      x = target_pretrain.sample((num_samples,)).to(device)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = model.forward_kld(x)\n",
        "\n",
        "      # Do backprop and optimizer step\n",
        "      if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "          loss.backward()\n",
        "          optimizer1.step()\n",
        "\n",
        "      if it % 100 == 99:\n",
        "        clear_output(wait=True)\n",
        "        print('Loss:', loss.item())\n",
        "\n",
        "  for mirror_itr in range(n_max_mirror_iterations):\n",
        "    flows = []\n",
        "    for i in range(num_layers):\n",
        "        param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True )\n",
        "        flows.append(nf.flows.AffineCouplingBlock(param_map))\n",
        "        flows.append(nf.flows.Permute(2, mode='swap'))\n",
        "\n",
        "    model2 = nf.NormalizingFlow(base, flows).to(device)\n",
        "    model2.load_state_dict(model.state_dict())\n",
        "    optimizer_lr_input = optimizer_lr * (outer_lr_factor**mirror_itr)\n",
        "    optimizer = torch.optim.Adam(model2.parameters(), lr=optimizer_lr_input, weight_decay=1e-5)\n",
        "    current_patient = 0\n",
        "    epcoh_min = 200\n",
        "\n",
        "    model2.train()\n",
        "    torch.cuda.synchronize()\n",
        "    start_epoch = time.time()\n",
        "\n",
        "    input_eta = (eta*(outer_eta_factor**mirror_itr))\n",
        "    for it in tqdm(range(n_max_iterations), disable = not verbose):\n",
        "      optimizer.zero_grad()\n",
        "      torch.manual_seed(set_seed + trail_num)\n",
        "      z = base.sample(num_samples)\n",
        "      log_prob_rho_0 = base.log_prob(z)\n",
        "      sampled_theta, log_det_model = model2.forward_and_log_det(z)\n",
        "      log_prob_model = log_prob_rho_0 - log_det_model\n",
        "      log_prob_prev = model.log_prob(sampled_theta)\n",
        "\n",
        "      L_n_loss = torch.mean( (1/4)*torch.linalg.vector_norm(sampled_theta, ord=2, dim=1).to(device) ** 4 + log_prob_model)\n",
        "      kld_loss = torch.mean(log_prob_model).to(torch.double) - torch.mean(log_prob_prev).to(torch.double)\n",
        "      kld_loss = kld_loss if kld_loss.item() >= 0 else torch.tensor([0.0]).to(device)\n",
        "      kld_loss = kld_loss if kld_loss.item() <= 5 else torch.tensor([5.0]).to(device)\n",
        "      loss = L_n_loss   + (1/input_eta)*kld_loss\n",
        "\n",
        "      # Do backprop and optimizer step\n",
        "      if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "        loss.backward()\n",
        "        grads = [param.grad.detach().flatten()\n",
        "            for param in model2.parameters()\n",
        "            if param.grad is not None]\n",
        "        norm = torch.cat(grads).norm()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      if verbose:\n",
        "        if it % 100 == 99:\n",
        "          # clear_output(wait=True)\n",
        "          print('trail:',trail_num,'m step:',mirror_itr,'Loss:', loss.item(), ' L_n:', L_n_loss.item(), ' kld:',kld_loss.item(),'norm:', norm.item())\n",
        "\n",
        "      if norm.item() > epcoh_min:\n",
        "        current_patient = current_patient + 1\n",
        "\n",
        "      if norm.item() < epcoh_min:\n",
        "        epcoh_min = norm.item()\n",
        "        current_patient = 0\n",
        "\n",
        "      if (current_patient >= patient_max) or (norm.item() < stopping_norm):\n",
        "        break\n",
        "\n",
        "    # After finishing the inner loop\n",
        "    torch.cuda.synchronize()\n",
        "    end_epoch = time.time()\n",
        "    elapsed = end_epoch - start_epoch\n",
        "    times = np.append(times, elapsed)\n",
        "\n",
        "    model.load_state_dict(model2.state_dict())\n",
        "\n",
        "\n",
        "    torch.manual_seed(set_seed + trail_num)\n",
        "    with torch.no_grad():\n",
        "      z = base.sample(num_samples)\n",
        "      log_prob_rho_0 = base.log_prob(z)\n",
        "      generated1, log_porb = model.forward_and_log_det(z)\n",
        "    x_all_data[mirror_itr,:,:] = generated1\n",
        "\n",
        "    L_n_loss_temp = L_n_loss\n",
        "    mirror_loss_hist = np.append(mirror_loss_hist, L_n_loss_temp.to('cpu').data.numpy())\n",
        "\n",
        "  # compute W1 loss\n",
        "  objective_fun_loss_list = np.array([])\n",
        "  for t in tqdm(range(n_max_mirror_iterations)):\n",
        "    x = x_all_data[t,:,:]\n",
        "    x_final = target_sample\n",
        "    x_data_rep = x.repeat(1,1,num_samples).reshape(num_samples,num_samples,-1)\n",
        "    cost = torch.norm(x_data_rep - x_final, p = 1, dim = 2).cpu().numpy()\n",
        "    row_ind, col_ind = linear_sum_assignment(cost)\n",
        "    temp_loss = (cost[row_ind, col_ind].sum() / num_samples )\n",
        "    objective_fun_loss_list = np.append(objective_fun_loss_list, temp_loss )\n",
        "\n",
        "  W1_2d[trail_num,:] = objective_fun_loss_list\n",
        "  Ln_rho_k_list[trail_num,:] = mirror_loss_hist\n",
        "  times_2d[trail_num:] = times"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}