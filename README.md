# Minimizing_Convex_Functionals_over_Space_of_Probability_Measures_via_KL_Divergence_Gradient_Flow

This repository contains code of the experiments of AISTATS 2024 paper [Minimizing Convex Functionals over Space of Probability Measures via KL Divergence Gradient Flow](https://arxiv.org/abs/2311.00894), Rentian Yao, Linjun Huang, and Yun Yang. We introduce an implicit scheme, called the implicit KL proximal descent (IKLPD) algorithm, for discretizing a continuous-time gradient flow relative to the Kullbackâ€“Leibler (KL) divergence for minimizing a convex target functional. We perform experiments to validate the effectiveness of our approach works in different scenarios.

## Description

An in-depth paragraph about your project and overview of use.


## Dependencies

* We use the [`normflows` package](https://github.com/VincentStimper/normalizing-flows/tree/master) to implement the IKLPD algorithm.
