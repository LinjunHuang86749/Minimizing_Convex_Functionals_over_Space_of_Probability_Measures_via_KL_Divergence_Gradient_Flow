{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cktzMYGRvRxW"
      },
      "outputs": [],
      "source": [
        "each_mirror_step_gen_theta_size = 5000\n",
        "given_data_size = 1000\n",
        "eta = 1 ###################################################\n",
        "total_trial_num = 1        # num trial\n",
        "n_max_mirror_iterations=50       # num mirror steps\n",
        "patient_max = 200           # max patient\n",
        "patient_max_mirror = 5\n",
        "stopping_norm = 1e-4\n",
        "num_samples = 1000         # number of theta generated\n",
        "verbose = True\n",
        "\n",
        "pretrain_factor = 1\n",
        "control_factor_1 = 1\n",
        "outer_lr_final_factor = 1e0\n",
        "outer_eta_final_factor = 1e0\n",
        "import math\n",
        "outer_lr_factor = pow(math.e, math.log(outer_lr_final_factor)/n_max_mirror_iterations)\n",
        "outer_eta_factor = pow(math.e, math.log(outer_eta_final_factor)/n_max_mirror_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o1tMGeAVeztY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e11b44b-923b-4cd0-eb97-55c92838f971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting normflows\n",
            "  Using cached normflows-1.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from normflows) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from normflows) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->normflows) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->normflows) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->normflows) (1.3.0)\n",
            "Installing collected packages: normflows\n",
            "Successfully installed normflows-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install normflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BTGAsyICe9iB"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import torch\n",
        "import numpy as np\n",
        "import normflows as nf\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as TD\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import gc\n",
        "import copy\n",
        "import time\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.stats import multivariate_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UD4fG8tffElv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(114514)\n",
        "# Move model on GPU if available\n",
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
        "sampled_theta_m = nf.distributions.CircularGaussianMixture(n_modes=4) # nf.distributions.TwoMoons().to(device)\n",
        "\n",
        "cov_mx = control_factor_1*torch.tensor([[1, 0], [0, 1.]], dtype=torch.double).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K9m9XC-GfnRH"
      },
      "outputs": [],
      "source": [
        "def find_L_n_faster(x, theta_list):\n",
        "  num_samples = theta_list.shape[0]\n",
        "  given_data_size = x.shape[0]\n",
        "\n",
        "  std_normal = TD.MultivariateNormal(\n",
        "            torch.zeros(num_samples, 2).to(device),\n",
        "            torch.eye(2).to(device).unsqueeze(0).repeat(num_samples, 1, 1),\n",
        "            validate_args=False)\n",
        "  x_data_rep = x.repeat(1,1,num_samples).reshape(given_data_size,num_samples,-1).to(torch.double).to(device)\n",
        "  prob_mx = torch.exp(std_normal.log_prob(theta_list - x_data_rep)).to(torch.double)\n",
        "  return -torch.mean(torch.log(torch.mean(prob_mx, dim = 1))).to(torch.double)\n",
        "\n",
        "def find_first_variation_var_faster(x, theta_list):\n",
        "  num_samples = theta_list.shape[0]\n",
        "  given_data_size = x.shape[0]\n",
        "\n",
        "  std_normal = TD.MultivariateNormal(\n",
        "            torch.zeros(num_samples, 2).to(device),\n",
        "            torch.eye(2).to(device).unsqueeze(0).repeat(num_samples, 1, 1),\n",
        "            validate_args=False)\n",
        "  x_data_rep = x.repeat(1,1,num_samples).reshape(given_data_size,num_samples,-1).to(device)\n",
        "  prob_mx = torch.exp(std_normal.log_prob(theta_list - x_data_rep))\n",
        "  row_mean = torch.transpose(torch.mean(prob_mx, dim = 1).repeat(num_samples, 1), 0, 1)\n",
        "  first_variation = -torch.mean(prob_mx/row_mean, dim = 0)\n",
        "  first_variation_var = torch.var(first_variation) # torch.var(first_variation)\n",
        "  return first_variation_var\n",
        "\n",
        "\n",
        "def find_first_variation_inner_var_faster(x, theta_list, tau, log_prob_model, log_prob_prev):\n",
        "  num_samples = theta_list.shape[0]\n",
        "  given_data_size = x.shape[0]\n",
        "\n",
        "  std_normal = TD.MultivariateNormal(\n",
        "            torch.zeros(num_samples, 2).to(device),\n",
        "            torch.eye(2).to(device).unsqueeze(0).repeat(num_samples, 1, 1),\n",
        "            validate_args=False)\n",
        "  x_data_rep = x.repeat(1,1,num_samples).reshape(given_data_size,num_samples,-1).to(device)\n",
        "  prob_mx = torch.exp(std_normal.log_prob(theta_list - x_data_rep))\n",
        "  row_mean = torch.transpose(torch.mean(prob_mx, dim = 1).repeat(num_samples, 1), 0, 1)\n",
        "  first_variation = -torch.mean(prob_mx/row_mean, dim = 0)\n",
        "  kld_loss_true = torch.mean(log_prob_model).to(torch.double) - torch.mean(log_prob_prev).to(torch.double)\n",
        "  sec_term = (1/tau) * (log_prob_model - log_prob_prev) # if kld_loss_true.item() >= 0 else torch.tensor([0.0]).to(device)\n",
        "  input = first_variation  + sec_term\n",
        "  first_variation_var = torch.var(input) # torch.var(input )\n",
        "  return first_variation_var\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 5\n",
        "n_max_iterations=5000\n",
        "FVV1_threshold_init = 0.07 # 0.07  ###################################################\n",
        "mirror_threshold = 0.05  ###################################################\n",
        "lr = 2e-3 # try 1e-5\n",
        "beta2 = 0.99  # 0.99default\n",
        "a = 20  # decay of lr wrt k\n",
        "b = 10  # decay of inner threshold wrt k\n",
        "dist = 1.4 # distance\n",
        "pretrain_factor = 1\n",
        "set_seed = 115"
      ],
      "metadata": {
        "id": "x8dU8h6ydbHb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jR6v6kGrzqYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4f9623-be54-4da7-8173-f9dc1b5ee20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 200/200 [00:03<00:00, 66.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.8190982341766357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 54/5000 [00:02<04:01, 20.49it/s]\n",
            "  1%|▏         | 64/5000 [00:03<04:05, 20.13it/s]\n",
            "  2%|▏         | 102/5000 [00:04<04:09, 19.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trail: 0 m step: 2 Loss: 4.85944009130675  L_n: 4.700374415571276 FVV1: 75.82891845703125 norm: 4.5869622230529785 FVV2: 0.4878273010253906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 203/5000 [00:09<03:48, 20.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trail: 0 m step: 2 Loss: 4.942940764399592  L_n: 4.606335000964228 FVV1: 0.27040454745292664 norm: 6.646571159362793 FVV2: 0.14801962673664093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 302/5000 [00:14<03:45, 20.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trail: 0 m step: 2 Loss: 4.288806239094983  L_n: 4.184845248189221 FVV1: 0.11233320087194443 norm: 3.289273262023926 FVV2: 0.09738747030496597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 396/5000 [00:19<03:43, 20.58it/s]\n",
            "  2%|▏         | 103/5000 [00:04<04:05, 19.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trail: 0 m step: 3 Loss: 4.3513730659280405  L_n: 4.171203292826234 FVV1: 0.22765524685382843 norm: 1.5003694295883179 FVV2: 0.08728974312543869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 197/5000 [00:09<03:51, 20.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished\n"
          ]
        }
      ],
      "source": [
        "Ln_rho_k_list = np.zeros((total_trial_num, n_max_mirror_iterations))\n",
        "times_2d = np.zeros((total_trial_num, n_max_mirror_iterations))\n",
        "iterations_2d = np.zeros((total_trial_num, n_max_mirror_iterations))\n",
        "\n",
        "sampled_theta_m = nf.distributions.TwoMoons().to(device)\n",
        "\n",
        "for trail_num in range(total_trial_num):\n",
        "  loss_hist = np.array([])\n",
        "  torch.manual_seed(set_seed + trail_num)\n",
        "  np.random.seed(set_seed + trail_num)\n",
        "\n",
        "\n",
        "  sampled_mean = dist*sampled_theta_m.sample(given_data_size).to(device)\n",
        "\n",
        "  normal_temp = TD.MultivariateNormal(\n",
        "        sampled_mean.to(device),\n",
        "        torch.eye(2).to(device).unsqueeze(0).repeat(given_data_size, 1, 1).to(device),\n",
        "        validate_args=False)\n",
        "\n",
        "  given_data = normal_temp.sample().detach().to(device)\n",
        "\n",
        "  mirror_loss_hist = np.zeros(n_max_mirror_iterations)\n",
        "  times = np.zeros(n_max_mirror_iterations)\n",
        "  iterations_1d = np.zeros(n_max_mirror_iterations)\n",
        "\n",
        "  target_pretrain = TD.MultivariateNormal(\n",
        "      torch.zeros(2).to(device), pretrain_factor * torch.eye(2).to(device))\n",
        "  # Define 2D Gaussian base distribution\n",
        "  base = nf.distributions.DiagGaussian(2, trainable=False)\n",
        "  # Define list of flows\n",
        "  num_layers = 10\n",
        "  flows = []\n",
        "  for i in range(num_layers):\n",
        "      # Neural network with two hidden layers having 64 units each\n",
        "      param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True )\n",
        "      flows.append(nf.flows.AffineCouplingBlock(param_map))\n",
        "      flows.append(nf.flows.Permute(2, mode='swap'))\n",
        "\n",
        "  model = nf.NormalizingFlow(base, flows).to(device)\n",
        "  optimizer1 = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3, betas=(0.9, 0.999))\n",
        "\n",
        "  for it in tqdm(range(200)):\n",
        "      optimizer1.zero_grad()\n",
        "\n",
        "      # Get training samples\n",
        "      x = target_pretrain.sample((num_samples,)).to(device)\n",
        "      loss = model.forward_kld(x)\n",
        "\n",
        "      # Do backprop and optimizer step\n",
        "      if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "          loss.backward()\n",
        "          optimizer1.step()\n",
        "\n",
        "      if it % 100 == 99:\n",
        "        clear_output(wait=True)\n",
        "        print('Loss:', loss.item())\n",
        "\n",
        "  current_mirror_patient = 0\n",
        "  mirror_min = 5000\n",
        "  mirror_min_prev = 5000\n",
        "  for mirror_itr in range(n_max_mirror_iterations):\n",
        "    optimizer_lr = a * lr/(mirror_itr + a)\n",
        "    inner_threshold =  b * FVV1_threshold_init/(mirror_itr + b)\n",
        "    flows = []\n",
        "    for i in range(num_layers):\n",
        "        param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True )\n",
        "        flows.append(nf.flows.AffineCouplingBlock(param_map))\n",
        "        flows.append(nf.flows.Permute(2, mode='swap'))\n",
        "\n",
        "\n",
        "    model2 = nf.NormalizingFlow(base, flows).to(device)\n",
        "    optimizer_lr_input = optimizer_lr\n",
        "    optimizer = torch.optim.Adam(model2.parameters(), lr=optimizer_lr_input, weight_decay=1e-5, betas=(0.9, beta2))\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
        "\n",
        "\n",
        "    model2.train()\n",
        "    torch.cuda.synchronize()\n",
        "    start_epoch = time.time()\n",
        "\n",
        "    input_eta = (eta*(outer_eta_factor**mirror_itr))\n",
        "    for it in tqdm(range(n_max_iterations), disable = not verbose):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      z = base.sample(num_samples)\n",
        "      log_prob_rho_0 = base.log_prob(z)\n",
        "      sampled_theta, log_det_model = model2.forward_and_log_det(z)\n",
        "      log_prob_model = log_prob_rho_0 - log_det_model\n",
        "      log_prob_prev = model.log_prob(sampled_theta)\n",
        "\n",
        "      L_n_loss = find_L_n_faster(x=given_data.to(torch.double), theta_list=sampled_theta.to(torch.double))\n",
        "      kld_loss_true = torch.mean(log_prob_model).to(torch.double) - torch.mean(log_prob_prev).to(torch.double)\n",
        "      kld_loss = kld_loss_true if kld_loss_true.item() >= 0 else torch.tensor([0.0]).to(device)\n",
        "      kld_loss = kld_loss if kld_loss.item() <= 500 else torch.tensor([500.0]).to(device)\n",
        "      loss = L_n_loss    + (1/input_eta)*kld_loss\n",
        "\n",
        "      # Do backprop and optimizer step\n",
        "      if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "        loss.backward()\n",
        "        grads = [param.grad.detach().flatten()\n",
        "            for param in model2.parameters()\n",
        "            if param.grad is not None]\n",
        "        norm = torch.cat(grads).norm()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      z = base.sample(num_samples)\n",
        "      log_prob_rho_0 = base.log_prob(z)\n",
        "      sampled_theta, log_det_model = model2.forward_and_log_det(z)\n",
        "      log_prob_model = log_prob_rho_0 - log_det_model\n",
        "      log_prob_prev = model.log_prob(sampled_theta)\n",
        "\n",
        "      FVV = find_first_variation_inner_var_faster(x=given_data, theta_list = sampled_theta, tau = input_eta, log_prob_model = log_prob_model, log_prob_prev = log_prob_prev)\n",
        "      FVV2 = find_first_variation_var_faster(x=given_data, theta_list = sampled_theta)\n",
        "      if verbose:\n",
        "        if it % 100 == 99:\n",
        "          # clear_output(wait=True)\n",
        "          print('trail:',trail_num,'m step:',mirror_itr,'Loss:', loss.item(), ' L_n:', L_n_loss.item(), 'FVV1:',FVV.item(),'norm:', norm.item(), 'FVV2:', FVV2.item())\n",
        "\n",
        "      if FVV.item() < inner_threshold:\n",
        "        break\n",
        "\n",
        "    # After finishing the inner loop\n",
        "    torch.cuda.synchronize()\n",
        "    end_epoch = time.time()\n",
        "    elapsed = end_epoch - start_epoch\n",
        "    times[mirror_itr] = elapsed\n",
        "\n",
        "    model.load_state_dict(model2.state_dict())\n",
        "\n",
        "    iterations_1d[mirror_itr]  = it + 1\n",
        "\n",
        "\n",
        "    torch.manual_seed(set_seed + trail_num)\n",
        "    with torch.no_grad():\n",
        "      z = base.sample(num_samples)\n",
        "      log_prob_rho_0 = base.log_prob(z)\n",
        "      generated1, log_porb = model.forward_and_log_det(z)\n",
        "    # x_all_data[mirror_itr,:,:] = generated1\n",
        "\n",
        "    L_n_loss_temp = L_n_loss\n",
        "    mirror_loss_hist[mirror_itr] = L_n_loss_temp.to('cpu').data.numpy()\n",
        "\n",
        "    FVV_mirror = find_first_variation_var_faster(x=given_data, theta_list = generated1)\n",
        "\n",
        "    if FVV_mirror.item() < mirror_threshold and (FVV.item() < inner_threshold) and (mirror_itr > 1):\n",
        "      print(\"finished\")\n",
        "      break\n",
        "    if FVV_mirror.item() > mirror_threshold and (FVV.item() > inner_threshold) and (mirror_itr > 1):\n",
        "      print(\"fail\")\n",
        "      break\n",
        "\n",
        "\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FVV, FVV_mirror, FVV_mirror.item(), FVV.item()"
      ],
      "metadata": {
        "id": "UG0ceuCZ6ekL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fa1863-5e86-4364-9762-c1d59c56b860"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0435, device='cuda:0', grad_fn=<VarBackward0>),\n",
              " tensor(0.0436, device='cuda:0'),\n",
              " 0.04359858110547066,\n",
              " 0.04345352202653885)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(sum([iterations_1d > 0]))"
      ],
      "metadata": {
        "id": "NCL4jPJhkKRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea7f20f-c0ef-4f39-9d28-a1ca1baa6500"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(iterations_1d[2:][iterations_1d[2:] > 0])"
      ],
      "metadata": {
        "id": "7shqYnOzAY9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2c486c-d0d5-4372-8156-d176d074b593"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "297.5"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}